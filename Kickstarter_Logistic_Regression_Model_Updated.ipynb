{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedirectory = os.getcwd()\n",
    "os.chdir(r\"C:\\Users\\ivann\\OneDrive\\Documents\\~General Assembly Documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"DSI_kickstarterscrape_dataset.csv\", encoding =\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()\n",
    "#two things you shoul always do, dataset.info() and dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raw_data.info())\n",
    "display(raw_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's update the data types on our raw data\n",
    "data_updated_dtype = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the project id from an integer to a string\n",
    "data_updated_dtype['project id'] = data_updated_dtype['project id'].astype('string')\n",
    "\n",
    "\n",
    "#let's put the day of the week into it's own column\n",
    "data_updated_dtype[['Day of Week','funded date']] = data_updated_dtype['funded date'].str.split(',',n=1,expand=True)\n",
    "#let's remove the annoying zeros at the end of the each entry\n",
    "data_updated_dtype['funded date']= data_updated_dtype['funded date'].str.rstrip('-0000')\n",
    "data_updated_dtype['Funded Date'] = pd.to_datetime(data_updated_dtype['funded date']).dt.date\n",
    "data_updated_dtype['Funded Time'] = pd.to_datetime(data_updated_dtype['funded date']).dt.time\n",
    "data_updated_dtype.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_updated_dtype.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's drop the original date time column\n",
    "data_updated_dtype.drop(columns = ['funded date'], inplace = True)\n",
    "data_updated_dtype.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_updated_dtype.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check to see what type of data we have\n",
    "data_updated_dtype.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_updated_dtype.isna().any() #let's check for missing values in our updated dtype data set\n",
    "#Expected output: \n",
    "# project id           False\n",
    "# name                 False\n",
    "# url                  False\n",
    "# category             False\n",
    "# subcategory          False\n",
    "# location              True\n",
    "# status               False\n",
    "# goal                 False\n",
    "# pledged               True\n",
    "# funded percentage    False\n",
    "# backers              False\n",
    "# levels               False\n",
    "# reward levels         True\n",
    "# updates              False\n",
    "# comments             False\n",
    "# duration             False\n",
    "# Day of Week          False\n",
    "# Funded Date          False\n",
    "# Funded Time          False\n",
    "# dtype: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename two word columns to be correct\n",
    "data_updated_dtype[\"reward_levels\"]=data_updated_dtype['reward levels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove old reward levels column\n",
    "data_updated_dtype.drop(columns=['reward levels'],inplace=True, axis=1)\n",
    "data_updated_dtype.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the number of null values in a specific column\n",
    "pledged_null = data_updated_dtype.isnull().pledged.sum()\n",
    "location_null = data_updated_dtype.isnull().location.sum()\n",
    "reward_null = data_updated_dtype.isnull().reward_levels.sum()\n",
    "print(\"null location count\", location_null)\n",
    "print(\"pledged null\", pledged_null)  \n",
    "print (\"reward levels null\", reward_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's drop any rows that are missing locations, since we have no way to determine what the Location should be from the data we have.\n",
    "data_updated_dtype.dropna(subset=['location'],inplace=True)\n",
    "\n",
    "#let's double check that we removed all the null values\n",
    "location_null = data_updated_dtype.isnull().location.sum()\n",
    "print(\"null location count\", location_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's drop the rows missing reward levels since we have no way to calculate this as well.\n",
    "data_updated_dtype.dropna(subset=['reward_levels'],inplace=True)\n",
    "\n",
    "#let's double check that we removed all the null values\n",
    "reward_null = data_updated_dtype.isnull().reward_levels.sum()\n",
    "print (\"reward levels null\", reward_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now attempt to update the null pldeged values using Goal amount and funded percentage\n",
    "#call all rows that do not contain a pledged amount\n",
    "#Let's calculate a new column called \"Calculted_Pledged\" and compare that with the reported pldege amounts to see if they are \n",
    "#the same.\n",
    "data_updated_dtype['Calculated_Pledged']=round(data_updated_dtype['goal']*(data_updated_dtype['funded percentage']),0)\n",
    "data_updated_dtype['pledgedMatch?'] = np.where(data_updated_dtype['pledged'] == data_updated_dtype['Calculated_Pledged'], 'True', 'False')  \n",
    "#create new column in df1 to check if pledged amounts  match\n",
    "\n",
    "data_updated_dtype.sample(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's add in the pldged amount where appropiate in our dataset\n",
    "nullpledged_df = data_updated_dtype[data_updated_dtype.isnull().pledged]\n",
    "nullpledged_df.head(20) #there should only be 12 rows in the pledged null data frame\n",
    "# data_updated_dtype['pledged']=round(nullpledged_df['goal']*(nullpledged_df['funded percentage']),0)\n",
    "# pledged_null = data_updated_dtype.isnull().pledged.sum()\n",
    "# print(\"pledged null\", pledged_null)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's attempt to fill in the values for this temporary data frame\n",
    "#the following code snippet was me testing how to use the np.where function, which essentialy acts like an if-then function.\n",
    "#The function worked the way that I hoped it would\n",
    "# nullpledged_df['pledged'] = np.where(nullpledged_df['pledged'].isnull(),round(nullpledged_df['goal']*(nullpledged_df['funded percentage']),0),nullpledged_df['pledged'])\n",
    "# nullpledged_df.head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the numpy \"where\" function to replace NAN values in the data set with by calculating pledged amount from fundraising goal * pledged percentage.\n",
    "data_updated_dtype['pledged'] = np.where(data_updated_dtype['pledged'].isnull(),round(data_updated_dtype['goal']*(data_updated_dtype['funded percentage']),0),data_updated_dtype['pledged'])\n",
    "pledged_null = data_updated_dtype.isnull().pledged.sum()\n",
    "#the expected output is 0. \n",
    "print(\"pledged null\", pledged_null) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataset to be used for further data cleaning\n",
    "#We need to drop the extra columns we created, i.e. pledgedmatch? and calculated pledge\n",
    "data_updated_dtype.drop(columns= ['Calculated_Pledged','pledgedMatch?'], inplace=True)\n",
    "no_nulls_data= data_updated_dtype\n",
    "no_nulls_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next Step: Let's make sure all categories are unique and consistent\n",
    "print(no_nulls_data.category.unique())\n",
    "print(no_nulls_data.groupby(['category'])['category'].count())\n",
    "#From our output, we see that Film & Video is written two ways, Film & Video + Film &amp; Video\n",
    "#The total expected amount of listings that fall in the Film & video category is 13,082\n",
    "#Expected output\n",
    "# category\n",
    "    # Art                  3872\n",
    "    # Comics               1034\n",
    "    # Dance                 744\n",
    "    # Design               1738\n",
    "    # Fashion              1117\n",
    "    # Film & Video          482\n",
    "    # Film &amp; Video    12600\n",
    "    # Food                 1411\n",
    "    # Games                1689\n",
    "    # Music               10671\n",
    "    # Photography          1424\n",
    "    # Publishing           4585\n",
    "    # Technology            774\n",
    "    # Theater              2451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relace all instances of Film &amp; Video with Film & Video\n",
    "no_nulls_data['category'] = np.where(no_nulls_data['category'] =='Film &amp; Video','Film & Video',no_nulls_data['category'])\n",
    "print(no_nulls_data.groupby(['category'])['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's identify the unique values of our subcategories\n",
    "# print(no_nulls_data.subcategory.unique())\n",
    "print(no_nulls_data.groupby(['subcategory'])['subcategory'].count())\n",
    "# print(no_nulls_data.loc[no_nulls_data['subcategory'] == 'Board & Card Games'].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's redefine Board &amp; Card Games,  Country &amp; Folk  ,  Film &amp; Video\n",
    "no_nulls_data['subcategory'] = np.where(no_nulls_data['subcategory'] =='Film &amp; Video','Film & Video',no_nulls_data['subcategory'])\n",
    "no_nulls_data['subcategory'] = np.where(no_nulls_data['subcategory'] =='Country &amp; Folk','Country & Folk',no_nulls_data['subcategory'])\n",
    "no_nulls_data['subcategory'] = np.where(no_nulls_data['subcategory'] =='Board &amp; Card Games','Board & Card Games',no_nulls_data['subcategory'])\n",
    "\n",
    "# for item  in no_nulls_data['subcategory']:\n",
    "#     if (item == 'Film &amp; Video'):\n",
    "#         no_nulls_data['subcategory'][item]= ('Film & Video')\n",
    "#     else:\n",
    "#         item = item\n",
    "    \n",
    "# no_nulls_data.shape\n",
    "#In order to iterate over an index I need to reindex my code.\n",
    "no_nulls_data.reset_index(drop=True, inplace= True)\n",
    "no_nulls_data.head(20)\n",
    "# print(no_nulls_data['subcategory'][0]);\n",
    "# print(no_nulls_data['subcategory'][11]);\n",
    "# i = 0;\n",
    "# for i in range(len(no_nulls_data['subcategory'])):\n",
    "# #     print(no_nulls_data['category'][i])\n",
    "#     if no_nulls_data['subcategory'][i] == 'Film &amp; Video':\n",
    "#         no_nulls_data['subcategory'][i] =\"Film & Video\"\n",
    "#     elif no_nulls_data['subcategory'][i] == 'Board &amp; Card Games':\n",
    "#         no_nulls_data['subcategory'][i] = \"Board & Card Games\"\n",
    "#     elif no_nulls_data['subcategory'][i] == 'Country &amp; Folk':\n",
    "#         no_nulls_data['subcategory'][i] =\"Country & Folk\"\n",
    "# no_nulls_data.column = df.column.apply(<urstuff>)\n",
    "    \n",
    "        \n",
    "        \n",
    "#     if no_nulls_data['subcategory'][i] == 'Film &amp; Video':\n",
    "#         no_nulls_data['subcategory'][i] = \"Film & video\"\n",
    "#     else:\n",
    "#         no_nulls_data['subcategory'][i] = no_nulls_data['subcategory'][i]\n",
    "# no_nulls_data.head(20)\n",
    "#Print statement to make sure everything was renamed properly\n",
    "#print(no_nulls_data.subcategory.unique())\n",
    "no_nulls_data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's remove extra columns if possible\n",
    "# print(no_nulls_data['pledgedMatch?'].is())\n",
    "# print(no_nulls_data['goal'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename no_null_data to the final dataset since everything is finally cleaned\n",
    "#Convert the the decimal repesentation of funded percentage to actually correspond with the percentage\n",
    "no_nulls_data['funded percentage']= no_nulls_data['funded percentage'].apply(lambda x: x*100)\n",
    "final_data=no_nulls_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's describe our data set \n",
    "round(final_data.describe(),2)\n",
    "#             goal   \tpledged  \tfunded percentage\tbackers \tlevels  \tupdates \tcomments\tduration\n",
    "# count\t4.459200e+04\t4.459200e+04\t44592.000\t44592.000  \t44592.000\t44592.000\t44592.000\t44592.000\n",
    "# mean\t1.210894e+04\t5.104144e+03\t165.889 \t71.038  \t8.047\t4.069\t8.526\t39.628\n",
    "# std\t1.916062e+05\t5.769624e+04\t7634.553\t698.999  \t4.247\t6.404\t176.651\t17.087\n",
    "# min\t5.000000e-01\t0.000000e+00\t0.000   \t0.000   \t1.000\t0.000\t0.000\t1.000\n",
    "# 25%\t1.800000e+03\t2.000000e+02\t4.500   \t5.000   \t5.000\t0.000\t0.000\t30.000\n",
    "# 50%\t4.000000e+03\t1.326000e+03\t100.000 \t23.000  \t8.000\t2.000\t0.000\t31.605\n",
    "# 75%\t1.000000e+04\t4.205000e+03\t111.667 \t60.000  \t10.000\t6.000\t3.000\t47.020\n",
    "# max\t2.147484e+07\t1.026684e+07\t1506600.000\t87142.000\t80.000\t149.000\t19311.000\t91.960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['status'].unique()\n",
    "final_data\n",
    "#let's remove the rows that have live, cancelled, or suspended\n",
    "\n",
    "# get names of indexes for which \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = final_data[(final_data['status'] == \"live\") | (final_data['status'] == 'canceled') | (final_data['status'] == 'suspended')].index \n",
    "  \n",
    "# drop these given row \n",
    "# indexes from dataFrame \n",
    "final_data.drop(index_names, inplace = True) \n",
    "final_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "final_data\n",
    "# final_data['status'].unique()\n",
    "#Remove the rows that are not failed or successfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure the value we are trying to calculate is binary\n",
    "sns.countplot(x = 'status', data = final_data)\n",
    "#check, each item is either successful or failed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's drop the reward levels column and the location column to make things easier for us.\n",
    "#we are removing project name because I currently have no way to quanitify how good or bad a project name when read by the audience, therefore, making it impossible for me to currently estimate it's impact.\n",
    "logreg_data = final_data.drop(columns=['location','reward_levels','url','project id','name','Day of Week','Funded Date','Funded Time'])\n",
    "logreg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am also removing funding percentage and pledged ammounts because they highly correlate with whether or not something was successful\n",
    "logreg_data = logreg_data.drop(columns=['funded percentage', 'pledged'])\n",
    "logreg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to encode variables, let's start by encoding the status varaible\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "status_cat = logreg_data['status']\n",
    "status_encoded = label_encoder.fit_transform(status_cat)\n",
    "status_encoded[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = successful / 0 = failed\n",
    "status_DF = pd.DataFrame(status_encoded, columns=['status_successful'])\n",
    "status_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cat = logreg_data['category']\n",
    "category_encoded = label_encoder.fit_transform(category_cat)\n",
    "category_encoded[0:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg_data.category.unique())\n",
    "#13 unique categories\n",
    "\n",
    "# print(len(logreg_data.subcategory.unique()))\n",
    "#49 subcategories\n",
    "\n",
    "\n",
    "# print(logreg_data.subcategory.unique())\n",
    "\n",
    "# ['Short Film' 'Board & Card Games' 'Documentary' 'Fashion' 'Music'\n",
    "#  'Illustration' 'Indie Rock' 'Dance' 'Nonfiction' 'Film & Video' 'Fiction'\n",
    "#  'Theater' 'Games' 'Art Book' 'Animation' 'Country & Folk' 'Comics'\n",
    "#  'Webseries' 'Technology' 'Performance Art' 'Narrative Film' 'Video Games'\n",
    "#  'Product Design' 'Painting' 'Photography' 'Conceptual Art' 'Rock' 'Jazz'\n",
    "#  'Classical Music' 'Food' 'Art' 'Pop' 'Journalism' 'Poetry' 'World Music'\n",
    "#  'Sculpture' 'Publishing' 'Electronic Music' 'Open Software'\n",
    "#  \"Children's Book\" 'Open Hardware' 'Graphic Design' 'Hip-Hop' 'Public Art'\n",
    "#  'Mixed Media' 'Periodical' 'Crafts' 'Design' 'Digital Art']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colt = ColumnTransformer(transformers=[('encoder', OneHotEncoder(),[14])],remainder =\"passthrough\")\n",
    "# logreg_data = pd.DataFrame(colt.fit_transform(logreg_data))\n",
    "# logreg_data.head()\n",
    "target_variable = logreg_data['status']\n",
    "print(target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = pd.get_dummies(logreg_data,columns=['category','subcategory','status'],drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"max_columns\", 70) #Showing only two columns\n",
    "dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dummy_data.corr())\n",
    "#Should I look into mutual information score or PCA?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the numbers associated with correlation, the closer to 1 a pair is, the more statistically correlated they are.\n",
    "correlation_matrix=dummy_data.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix.loc[correlation_matrix['category_Design'] > 0.2,['category_Design']]\n",
    "#we see that design subcategories are highly correlated to their category of design, this makes sense a projects limits the what subcategories a project could fit into.\n",
    "#Therefore, I'm going to drop the category attribute while running my logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out the values that we need to remove from the dummy data\n",
    "category_list= correlation_matrix.filter(regex = '^category').iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns_to_drop= list(category_list.index.values)\n",
    "category_columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's drop time of day and funded date because I don't know how to account for these things yet\n",
    "dummy_data= dummy_data.drop(columns= category_columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy_data.columns\n",
    "#Double check to make sure category has been removed sucessfully, given our output we are given what we expect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dummy_data.drop('status_successful', axis=1),\n",
    "                                                   dummy_data['status_successful'], test_size=0.2,\n",
    "                                                   random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Apply feature scaling to the model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look up what these functions do\n",
    "LogReg = LogisticRegression(solver='liblinear')\n",
    "LogReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look up what these functions do\n",
    "y_pred = LogReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Model Evaluation\n",
    "### Classification report without cross-validation\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-fold cross-validation & confusion matrices\n",
    "#calculate the prediction score for your training set\n",
    "y_train_pred = cross_val_predict(LogReg, X_train, y_train, cv=5)\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LogReg.coef_)\n",
    "# coefficients = pd.Dataframe\n",
    "print(dummy_data.columns)\n",
    "column_names = dummy_data.drop('status_successful', axis=1).columns\n",
    "coeff_df = pd.DataFrame(LogReg.coef_, columns = column_names)\n",
    "coeff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets reshape the matrix so that it's one column and multiple rows.\n",
    "coeff_df_transpose= coeff_df.transpose()\n",
    "print(coeff_df_transpose)\n",
    "#next step, let's sort the rows by their value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df_transpose= coeff_df_transpose.rename(columns={0:\"Weights with Backers\"}, errors=\"raise\")\n",
    "print(coeff_df_transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df_transpose.sort_values('Weights with Backers')\n",
    "#backers has the highest positive weight of 38,\n",
    "#Comments had the positive wieght of 4,\n",
    "#updates had a positive weight of 1.207\n",
    "#Goal amount had a negative weight of -32.5, followed by duration at -0.2863\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull out category weights\n",
    "category_weights = coeff_df_transpose.filter(regex = \"^category\", axis=0)\n",
    "category_weights.sort_values('Weights with Backers', ascending = False)\n",
    "#Having the category of either Music, theater, or dance had a net positive effect,\n",
    "#while everything else had a negative effect on the ability to succeed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull out subcategory weights\n",
    "subcategory_weights = coeff_df_transpose.filter(regex = \"^subcategory\", axis=0)\n",
    "subcategory_weights.sort_values('Weights with Backers', ascending = False)\n",
    "#Short film had the most postiive effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see what we get when you pull-out category and the number of backers from our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dummies is function built into pandas that will create binary columns for us for categorical variables.\n",
    "\n",
    "dummy_data = pd.get_dummies(logreg_data,columns=['subcategory','status'],drop_first = True)\n",
    "#Let's drop time of day and funded date because I don't know how to account for these things yet\n",
    "#let's also get rid of backers because of how strong of an impact it had on everything. \n",
    "#Let's get rid of category to see just how important subcategory is for this analysis\n",
    "dummy_data= dummy_data.drop(columns= [\"backers\",'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dummy_data.drop('status_successful', axis=1),\n",
    "                                                   dummy_data['status_successful'], test_size=0.2,\n",
    "                                                   random_state=200)\n",
    "print(X_train.shape)\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "LogReg = LogisticRegression(solver='liblinear')\n",
    "LogReg.fit(X_train, y_train)\n",
    "y_pred = LogReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation\n",
    "### Classification report without cross-validation\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-fold cross-validation & confusion matrices\n",
    "#calculate the prediction score for your training set\n",
    "y_train_pred = cross_val_predict(LogReg, X_train, y_train, cv=5)\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save my coefficients into a pretty dataframe\n",
    "column_names = dummy_data.drop('status_successful', axis=1).columns\n",
    "no_backers_coeff_df = pd.DataFrame(LogReg.coef_, columns = column_names)\n",
    "no_backers_coeff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = {'backers': 'NA'}\n",
    "no_backers_coeff_df = no_backers_coeff_df.append(df2,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transpose the dataframe to make it easier to read\n",
    "\n",
    "\n",
    "no_backers_coeff_df_transpose= no_backers_coeff_df.transpose()\n",
    "\n",
    "no_backers_coeff_df_transpose.drop(columns = [1],inplace=True)\n",
    "\n",
    "\n",
    "no_backers_coeff_df_transpose= no_backers_coeff_df_transpose.rename(columns={0:\"Weights with No Backers\"}, errors=\"raise\")\n",
    "no_backers_coeff_df_transpose.sort_values('Weights with No Backers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take the weights from the training model before and after we removed # of backers and categories from the equation\n",
    "#Let's see how it differs.\n",
    "weight_comparison = pd.merge(coeff_df_transpose, no_backers_coeff_df_transpose, left_index= True, right_index=True)\n",
    "weight_comparison.sort_values('Weights with Backers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python385jvsc74a57bd0eeda52a798dd57b2ef8e13888075653d22071a8bc304d44073bbf5a37bcf9649"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}